{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network Model\n",
    "\n",
    "### IF3170 - Machine Learning\n",
    "\n",
    "Developed by: \n",
    "1. Juan Christopher Santoso (13521116)\n",
    "2. Nicholas Liem (13521135)\n",
    "3. Nathania Calista Djunaedi (13521139)\n",
    "4. Antonio Natthan Krishna (13521162)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Neural Network Properties**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    def __init__(self, types='Sigmoid'):\n",
    "        self.func = self.sigmoid\n",
    "        self.dfunc = self.dsigmoid\n",
    "        self.dfuncerr = self.dsum_square\n",
    "\n",
    "        match types:\n",
    "            case 'sigmoid':\n",
    "                self.func = self.sigmoid\n",
    "                self.dfunc = self.dsigmoid\n",
    "                self.dfuncerr = self.dsum_square\n",
    "            case 'linear':\n",
    "                self.func = self.linear\n",
    "                self.dfunc = self.dlinear\n",
    "                self.dfuncerr = self.dsum_square\n",
    "            case 'softmax':\n",
    "                self.func = self.softmax\n",
    "                self.dfunc = self.dsoftmax\n",
    "                self.dfuncerr = self.derr_softmax\n",
    "            case 'relu':\n",
    "                self.func = self.relu\n",
    "                self.dfunc = self.drelu\n",
    "                self.dfuncerr = self.dsum_square\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def dsigmoid(self, x):\n",
    "        sig = self.sigmoid(x)\n",
    "        return sig * (1-sig)\n",
    "    \n",
    "    def linear(self, x):\n",
    "        return x\n",
    "    \n",
    "    def dlinear(self, x):\n",
    "        return 1\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        expX = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return expX / np.sum(expX, axis=1, keepdims=True)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def drelu(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def dsum_square(self, output, target):\n",
    "        return target - output\n",
    "    \n",
    "    def derr_softmax(self, output, target):\n",
    "        if (target != 1):\n",
    "            return output\n",
    "        else:\n",
    "            return output - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def calculate(output, target, layers):\n",
    "        activation_mode = layers[-1]['activation_function']\n",
    "        if (activation_mode == \"softmax\"):\n",
    "            return LossFunction.loss_softmax(output, target)\n",
    "        else:\n",
    "            return LossFunction.loss_rsl(output, target)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_rsl(output, target):\n",
    "        err = 0\n",
    "        for i in range (len(output)):\n",
    "            err += (target[i] - output[i])**2\n",
    "        return err\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_softmax(output, target):\n",
    "        idx = np.where(target == 1)[0]\n",
    "        return -1 * np.log10(output[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Forward Propagation (Using Fast Forward Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardPropagation:\n",
    "    @staticmethod\n",
    "    def process(input_data, layers, weights):\n",
    "        activations = input_data\n",
    "        neuron_net = []\n",
    "        neuron_out = []\n",
    "\n",
    "        for i in range(len(layers)):\n",
    "            activations_with_bias = np.insert(activations, 0, 1, axis=1)\n",
    "            net_input = np.dot(activations_with_bias, weights[i])\n",
    "            activation_mode = layers[i]['activation_function']\n",
    "            activationFunc = ActivationFunction(activation_mode)\n",
    "            activations = activationFunc.func(net_input)\n",
    "            neuron_net.append(net_input)\n",
    "            neuron_out.append(activations)\n",
    "\n",
    "        return activations, neuron_net, neuron_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackwardPropagation:\n",
    "    @staticmethod\n",
    "    def process(weights, output, target, neuron_net, neuron_out, layers, learning_rate, input_data):\n",
    "        delta = []\n",
    "\n",
    "        # Delta Output Layer\n",
    "        activation_mode = layers[-1]['activation_function']\n",
    "        delta_layer = []\n",
    "        activationFunc = ActivationFunction(activation_mode)\n",
    "        if (activation_mode == 'softmax'):\n",
    "            for i in range (len(output)): \n",
    "                delta_layer.append(activationFunc.dfuncerr(output[i], target[i]))\n",
    "        else:\n",
    "            for i in range (len(output)): \n",
    "                delta_layer.append(activationFunc.dfuncerr(output[i], target[i]) * activationFunc.dfunc(output[i]))\n",
    "        delta.append(delta_layer)\n",
    "\n",
    "        # Delta Hidden Layer\n",
    "        for i in range(len(layers) - 2, -1, -1):\n",
    "            activation_mode = layers[i]['activation_function']\n",
    "            activationFunc = ActivationFunction(activation_mode)\n",
    "            prev_delta = delta_layer if i == len(layers) - 2 else delta_layer[1:]\n",
    "            delta_layer = []\n",
    "            layer_weight = weights[i+1]\n",
    "            layer_output = neuron_out[i][0]\n",
    "\n",
    "            for j in range (len(layer_weight)): \n",
    "                neuron_weight = layer_weight[j]\n",
    "                sigma = np.dot(neuron_weight, prev_delta)\n",
    "                delta_layer.append(sigma * activationFunc.dfunc(1 if j == 0 else layer_output[j-1]))\n",
    "            delta = [delta_layer[1:]] + delta\n",
    "\n",
    "        # print(delta)\n",
    "\n",
    "        # Update Weight\n",
    "        for i in range (len(layers)):\n",
    "            if (i == 0):\n",
    "                layer_input = input_data\n",
    "            else :\n",
    "                layer_input = neuron_out[i-1][0]\n",
    "\n",
    "            layer_input = np.insert(layer_input, 0, 1)\n",
    "            for j in range (len(weights[i])):\n",
    "                for k in range (len(weights[i][j])):\n",
    "                    weights[i][j][k] += learning_rate * delta[i][k] * layer_input[j]\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Artificial Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialNeuralNetwork:\n",
    "    def __init__(self, architecture, input_data, output):\n",
    "        self.layers = architecture.layers\n",
    "        self.learning_rate = architecture.learning_rate\n",
    "        self.error_threshold = architecture.error_threshold\n",
    "        self.max_iter = architecture.max_iter\n",
    "        self.batch_size = architecture.batch_size\n",
    "        self.input_data = input_data.values\n",
    "        self.output = output.values\n",
    "\n",
    "        # Processing Target\n",
    "        self.target = np.array([])\n",
    "        for item in self.output:\n",
    "            if item == \"Iris-setosa\":\n",
    "                self.target = np.append(self.target, [1, 0, 0])\n",
    "            elif item == \"Iris-versicolor\":\n",
    "                self.target = np.append(self.target, [0, 1, 0])\n",
    "            else:  # item == \"Iris-virginica\"\n",
    "                self.target = np.append(self.target, [0, 0, 1])\n",
    "        self.target = self.target.reshape(-1, 3)\n",
    "\n",
    "        # Initializing Weight\n",
    "        neurons_per_layer = [layer[\"number_of_neurons\"] for layer in self.layers]\n",
    "        self.weights = [[[1] * neurons_per_layer[0] for _ in range(len(self.input_data[0]) + 1)]]\n",
    "        for i in range(1, len(neurons_per_layer)):\n",
    "            prev_layer_neurons = neurons_per_layer[i - 1]\n",
    "            current_layer_neurons = neurons_per_layer[i]\n",
    "            self.weights.append([[1] * current_layer_neurons for _ in range(prev_layer_neurons + 1)])\n",
    "\n",
    "    def train(self):\n",
    "        temp_weight = copy.deepcopy(self.weights)\n",
    "\n",
    "        for i in range (self.max_iter):\n",
    "            error_total = 0\n",
    "            minibatch = self.batch_size\n",
    "            for i in range (len(self.input_data)):\n",
    "                if (minibatch == 0):\n",
    "                    self.weights = copy.deepcopy(temp_weight)\n",
    "                    minibatch = self.batch_size\n",
    "                output, neuron_net, neuron_out = ForwardPropagation.process([self.input_data[i]], self.layers, self.weights)\n",
    "                error_total += LossFunction.calculate(output[0], self.target[i], self.layers)\n",
    "                temp_weight = BackwardPropagation.process(temp_weight, output[0], self.target[i], neuron_net, neuron_out, self.layers, self.learning_rate, self.input_data[i])\n",
    "                minibatch = minibatch - 1\n",
    "\n",
    "            if (error_total < self.error_threshold):\n",
    "                break\n",
    "        self.weights = copy.deepcopy(temp_weight)\n",
    "        \n",
    "        print(self.weights)\n",
    "        print(error_total)\n",
    "\n",
    "    def predict(self, test):\n",
    "        res = []\n",
    "        for i in range (len(self.input_data)):\n",
    "            output, _, _ = ForwardPropagation.process([test.values[i]], self.layers, self.weights)\n",
    "            res.append(output)\n",
    "\n",
    "        self.test = []\n",
    "        for item in res:\n",
    "            max_index = np.argmax(item)\n",
    "            if (max_index == 0):\n",
    "                self.test.append(\"Iris-setosa\")\n",
    "            elif (max_index == 1):\n",
    "                self.test.append(\"Iris-versicolor\")\n",
    "            else:\n",
    "                self.test.append(\"Iris-virginica\")\n",
    "        print(self.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocessing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonModelParser:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.data = self.load_json_file()\n",
    "        self.parse_model_data()\n",
    "\n",
    "    def load_json_file(self):\n",
    "        try:\n",
    "            with open(self.filepath, 'r', encoding='utf-8') as file:\n",
    "                return json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"The file {self.filepath} was not found\")\n",
    "            return None\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON from the file {self.filepath}\")\n",
    "            return None\n",
    "\n",
    "    def parse_model_data(self):\n",
    "        if self.data:\n",
    "            self.model = self.data.get('model', {})\n",
    "            raw_layers = self.model.get('layers', [])   \n",
    "            self.layers = [{'number_of_neurons': layer.get('number_of_neurons'),\n",
    "                        'activation_function': layer.get('activation_function')}\n",
    "                       for layer in raw_layers]\n",
    "            self.learning_rate = self.model.get('learning_rate')\n",
    "            self.error_threshold = self.model.get('error_threshold')\n",
    "            self.max_iter = self.model.get('max_iter')\n",
    "            self.batch_size = self.model.get('batch_size')\n",
    "\n",
    "    @staticmethod\n",
    "    def save_json_file(data, filepath):\n",
    "        try:\n",
    "            with open(filepath, 'w', encoding='utf-8') as file:\n",
    "                json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        except IOError:\n",
    "            print(f\"Could not save data to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.csv')\n",
    "input = df.drop(columns=['Id', 'Species'])\n",
    "target = df['Species']\n",
    "architecture = JsonModelParser(\"arch_example.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Processing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ArtificialNeuralNetwork(architecture, input, target)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.7253086569122815, 0.7253086569122815, 0.7253086569122815], [-0.4112827310985512, -0.4112827310985512, -0.4112827310985512], [0.11524989843090758, 0.11524989843090758, 0.11524989843090758], [0.4133830755099026, 0.4133830755099026, 0.4133830755099026], [0.8628838393854685, 0.8628838393854685, 0.8628838393854685]], [[0.7822149361452975, 0.7822149361452975, 0.7822149361452975, 0.7822149361452975], [0.37693611886528805, 0.37693611886528805, 0.37693611886528805, 0.37693611886528805], [0.37693611886528805, 0.37693611886528805, 0.37693611886528805, 0.37693611886528805], [0.37693611886528805, 0.37693611886528805, 0.37693611886528805, 0.37693611886528805]], [[1.2001321446067714, 0.35574644283098045, -0.21421956238996828], [-0.11781815874836277, 0.0005528317068711164, 0.06437544127429624], [-0.11781815874836277, 0.0005528317068711164, 0.06437544127429624], [-0.11781815874836277, 0.0005528317068711164, 0.06437544127429624], [-0.11781815874836277, 0.0005528317068711164, 0.06437544127429624]]]\n",
      "['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "print(model.weights)\n",
    "model.predict(input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
