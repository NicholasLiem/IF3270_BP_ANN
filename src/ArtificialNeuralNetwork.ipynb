{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network Model\n",
    "\n",
    "### IF3170 - Machine Learning\n",
    "\n",
    "Developed by: \n",
    "1. Juan Christopher Santoso (13521116)\n",
    "2. Nicholas Liem (13521135)\n",
    "3. Nathania Calista Djunaedi (13521139)\n",
    "4. Antonio Natthan Krishna (13521162)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1660,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **JsonModelParser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1661,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonModelParser:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.data = self.load_json_file()\n",
    "        self.parse_model_data()\n",
    "\n",
    "    def printDetails(self):\n",
    "        print(\"\\tINPUT SIZE:\",self.input_size)\n",
    "        print(\"\\tLAYERS:\", self.layers)\n",
    "        print(\"\\tINPUT:\", self.input)\n",
    "        print(\"\\tINITIAL WEIGHTS:\", self.initial_weights)\n",
    "        print(\"\\tTARGET:\", self.target)\n",
    "        print(\"\\tLEARNING RATE:\", self.learning_rate)\n",
    "        print(\"\\tBATCH SIZE:\", self.batch_size)\n",
    "        print(\"\\tMAX ITERATION:\", self.max_iteration)\n",
    "        print(\"\\tERROR THRESHOLD:\", self.error_threshold)\n",
    "    \n",
    "    def load_json_file(self):\n",
    "        try:\n",
    "            with open(self.filepath, 'r', encoding='utf-8') as file:\n",
    "                return json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"The file {self.filepath} was not found\")\n",
    "            return None\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON from the file {self.filepath}\")\n",
    "            return None\n",
    "\n",
    "    def parse_model_data(self):\n",
    "        if self.data:\n",
    "            self.case = self.data.get('case', {})\n",
    "            self.model = self.case.get('model', {})\n",
    "            self.input_size = self.model.get('input_size')\n",
    "\n",
    "            raw_layers = self.model.get('layers', [])   \n",
    "            self.layers = [{'number_of_neurons': layer.get('number_of_neurons'),\n",
    "                        'activation_function': layer.get('activation_function')}\n",
    "                       for layer in raw_layers]\n",
    "            \n",
    "            self.input = self.case.get('input', [])\n",
    "            self.initial_weights = self.case.get('initial_weights', [])\n",
    "            self.target = self.case.get('target', [])\n",
    "            self.parameters = self.case.get('learning_parameters', {})\n",
    "\n",
    "            self.learning_rate = self.parameters.get('learning_rate')\n",
    "            self.batch_size = self.parameters.get('batch_size')\n",
    "            self.max_iteration = self.parameters.get('max_iteration')\n",
    "            self.error_threshold = self.parameters.get('error_threshold')\n",
    "\n",
    "            self.expect = self.data.get('expect', {})\n",
    "            self.stopped_by = self.expect.get('stopped_by', '')\n",
    "            self.final_weights = self.expect.get('final_weights', [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Neural Network Properties**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1662,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    def __init__(self, types='Sigmoid'):\n",
    "        self.func = self.sigmoid\n",
    "        self.dfunc = self.dsigmoid\n",
    "        self.dfuncerr = self.dsum_square\n",
    "\n",
    "        match types:\n",
    "            case 'sigmoid':\n",
    "                self.func = self.sigmoid\n",
    "                self.dfunc = self.dsigmoid\n",
    "                self.dfuncerr = self.dsum_square\n",
    "            case 'linear':\n",
    "                self.func = self.linear\n",
    "                self.dfunc = self.dlinear\n",
    "                self.dfuncerr = self.dsum_square\n",
    "            case 'softmax':\n",
    "                self.func = self.softmax\n",
    "                # self.dfuncerr = self.derr_softmax\n",
    "                self.dfuncerr = self.dsum_square\n",
    "            case 'relu':\n",
    "                self.func = self.relu\n",
    "                self.dfunc = self.drelu\n",
    "                self.dfuncerr = self.dsum_square\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def dsigmoid(self, x):\n",
    "        sig = self.sigmoid(x)\n",
    "        return sig * (1-sig)\n",
    "    \n",
    "    def linear(self, x):\n",
    "        return x\n",
    "    \n",
    "    def dlinear(self, x):\n",
    "        return 1\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        expX = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return expX / np.sum(expX, axis=1, keepdims=True)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def drelu(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def dsum_square(self, output, target):\n",
    "        return target - output\n",
    "    \n",
    "    def derr_softmax(self, output, target):\n",
    "        if (target != 1):\n",
    "            return output\n",
    "        else:\n",
    "            return output - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1663,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def calculate(output, target, layers):\n",
    "        activation_mode = layers[-1]['activation_function']\n",
    "        if (activation_mode == \"softmax\"):\n",
    "            return LossFunction.loss_softmax(output, target)\n",
    "        else:\n",
    "            return LossFunction.loss_rsl(output, target)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_rsl(output, target):\n",
    "        err = 0\n",
    "        for i in range (len(output)):\n",
    "            err += (target[i] - output[i])**2\n",
    "        return 0.5 * err\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def loss_softmax(output, target):\n",
    "        # idx = np.argmax(target)\n",
    "        # return -1* np.log10(output[idx])\n",
    "\n",
    "        err = 0\n",
    "        for i in range(len(output)):\n",
    "            err += -1* target[i] * np.log(output[i])\n",
    "        return err\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Forward Propagation (Using Fast Forward Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1664,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardPropagation:\n",
    "    @staticmethod\n",
    "    def process(input_data, layers, weights):\n",
    "        activations = input_data\n",
    "        neuron_net = []\n",
    "        neuron_out = []\n",
    "\n",
    "        for i in range(len(layers)):\n",
    "            activations_with_bias = np.insert(activations, 0, 1, axis=1)\n",
    "            net_input = np.dot(activations_with_bias, weights[i])\n",
    "            activation_mode = layers[i]['activation_function']\n",
    "            activationFunc = ActivationFunction(activation_mode)\n",
    "            activations = activationFunc.func(net_input)\n",
    "            neuron_net.append(net_input)\n",
    "            neuron_out.append(activations)\n",
    "\n",
    "        return activations, neuron_net, neuron_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1665,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackwardPropagation:\n",
    "    @staticmethod\n",
    "    def process(weights, output, target, neuron_out, layers, learning_rate, input_data):\n",
    "        \n",
    "        # Initiate variables\n",
    "        delta = []\n",
    "        delta_layer = []\n",
    "\n",
    "        # Delta Output Layer\n",
    "        activation_mode = layers[-1]['activation_function']\n",
    "        activationFunc = ActivationFunction(activation_mode)\n",
    "\n",
    "        # Handle Case for Softmax\n",
    "        if (activation_mode == 'softmax'):\n",
    "            for i in range (len(output)): \n",
    "                delta_layer.append(activationFunc.dfuncerr(output[i], target[i]))\n",
    "        else:\n",
    "            for i in range (len(output)): \n",
    "                delta_layer.append(activationFunc.dfuncerr(output[i], target[i]) * activationFunc.dfunc(output[i]))\n",
    "        delta.append(delta_layer)\n",
    "\n",
    "        # Delta Hidden Layer\n",
    "        for i in range(len(layers) - 2, -1, -1):\n",
    "            activation_mode = layers[i]['activation_function']\n",
    "            activationFunc = ActivationFunction(activation_mode)\n",
    "\n",
    "            # Save previous delta layer\n",
    "            # If 2nd last, then return the Output layer\n",
    "            # Else, return the previous layer but exclude the Bias val\n",
    "            prev_delta_layer = delta_layer if (i == (len(layers)-2)) else delta_layer[1:]\n",
    "\n",
    "            # Initiate new Delta Layer\n",
    "            delta_layer = []\n",
    "            layer_weight = weights[i+1]\n",
    "            layer_output = neuron_out[i][0]\n",
    "\n",
    "            # Iterate each neuron\n",
    "            for j in range (len(layer_weight)): \n",
    "                neuron_weight = layer_weight[j]\n",
    "                sigma = np.dot(neuron_weight, prev_delta_layer)\n",
    "\n",
    "                # Make sure hidden layers are not softmax\n",
    "                assert activation_mode != \"softmax\", \"Softmax cannot be in hidden layers\"\n",
    "                delta_layer.append(sigma * activationFunc.dfunc(1 if j == 0 else layer_output[j-1]))\n",
    "            # Append but push from front\n",
    "            delta = [delta_layer[1:]] + delta\n",
    "\n",
    "        # Update Weight\n",
    "        # Iterate all the layers\n",
    "        for i in range (len(layers)):\n",
    "            if (i == 0):\n",
    "                layer_input = input_data\n",
    "            else :\n",
    "                layer_input = neuron_out[i-1][0] # [0] because it is a list inside a list\n",
    "            layer_input = np.insert(layer_input, 0, 1)\n",
    "            \n",
    "            # Iterate all weights\n",
    "            for j in range (len(weights[i])):\n",
    "                # Iterate all weights\n",
    "                for k in range (len(weights[i][j])):\n",
    "                    # It is based on formula wji = wji + learn_rate * dE/dnet * dnet/dw\n",
    "                    weights[i][j][k] += learning_rate * (delta[i][k] * layer_input[j])\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Artificial Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1666,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialNeuralNetwork:\n",
    "    def __init__(self, architecture = None):\n",
    "      if architecture != None:\n",
    "        self.layers = architecture.layers\n",
    "        self.input_size = architecture.input_size\n",
    "        self.learning_rate = architecture.learning_rate\n",
    "        self.error_threshold = architecture.error_threshold\n",
    "        self.max_iter = architecture.max_iteration\n",
    "        self.batch_size = architecture.batch_size\n",
    "        self.input_data = architecture.input\n",
    "        self.target = architecture.target\n",
    "        self.weights = architecture.initial_weights\n",
    "        self.expected_stopped_by = architecture.stopped_by\n",
    "        self.expect_weights = architecture.final_weights\n",
    "\n",
    "    def predict(self):\n",
    "        res = []\n",
    "        for i in range (len(self.input_data)):\n",
    "            output, _, _ = ForwardPropagation.process([self.input_data[i]], self.layers, self.weights)\n",
    "            res.append(output)\n",
    "        return res\n",
    "\n",
    "    def train(self):\n",
    "        temp_weight = copy.deepcopy(self.weights)\n",
    "\n",
    "        for j in range (self.max_iter):\n",
    "            # Set initial error_total. Reset every iteration\n",
    "            error_total = 0\n",
    "            minibatch = self.batch_size\n",
    "\n",
    "            # batches_amount = math.ceil(len(self.input_data)/ minibatch)\n",
    "\n",
    "            for i in range (len(self.input_data)):\n",
    "                if (minibatch == 0):\n",
    "                    self.weights = copy.deepcopy(temp_weight)\n",
    "                    minibatch = self.batch_size\n",
    "                # Forward Propagation\n",
    "                output, _, neuron_out = ForwardPropagation.process([self.input_data[i]], self.layers, self.weights)\n",
    "                # Calculate Loss\n",
    "                error_total += LossFunction.calculate(output[0], self.target[i], self.layers)\n",
    "                # Backward Propagation\n",
    "                temp_weight = BackwardPropagation.process(temp_weight, output[0], self.target[i], neuron_out, self.layers, self.learning_rate, self.input_data[i])\n",
    "                minibatch = minibatch - 1\n",
    "            \n",
    "            self.weights = copy.deepcopy(temp_weight)\n",
    "\n",
    "            # Adjust error to mean value\n",
    "            error_total /= len(self.input_data)\n",
    "\n",
    "            if (error_total < self.error_threshold):\n",
    "                break        \n",
    "\n",
    "        if (j == self.max_iter-1):\n",
    "            self.stopped_by = \"MAX ITERATION\"\n",
    "        elif (error_total < self.error_threshold):\n",
    "            self.stopped_by = \"ERROR THRESHOLD\"\n",
    "        else:\n",
    "            self.stopped_by = \"UNIDENTIFIED\"\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"LAST ITERATION :\", j)\n",
    "        print(\"TOTAL ERROR VAL:\", error_total)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"EXCPECTED\")\n",
    "        print(\"STOPPED BY :\", self.expected_stopped_by)\n",
    "        for weight_group in self.expect_weights:\n",
    "            print(\"[\")\n",
    "            for weight in weight_group:\n",
    "                print(\"\\t\", weight)\n",
    "            print(\"]\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        print(\"RESULT\")\n",
    "        print(\"STOPPED BY :\", self.stopped_by)\n",
    "        for weight_group in self.weights:\n",
    "            print(\"[\")\n",
    "            for weight in weight_group:\n",
    "                print(\"\\t\", weight)\n",
    "            print(\"]\")\n",
    "    \n",
    "    def save_model(self, file_path):\n",
    "      data = {\n",
    "          \"case\": {\n",
    "              \"model\": {\n",
    "                  \"input_size\": self.input_size,\n",
    "                  \"layers\": self.layers\n",
    "              },\n",
    "              \"initial_weights\": self.weights,\n",
    "              \"learning_parameters\": {\n",
    "                  \"learning_rate\": self.learning_rate,\n",
    "                  \"batch_size\": self.batch_size,\n",
    "                  \"max_iteration\": self.max_iter,\n",
    "                  \"error_threshold\": self.error_threshold\n",
    "              }\n",
    "          }\n",
    "      }\n",
    "      with open(file_path, 'w') as json_file:\n",
    "          json.dump(data, json_file, indent=4)\n",
    "  \n",
    "    def load_model(self, file_path):\n",
    "      architecture = JsonModelParser(file_path)\n",
    "      self.layers = architecture.layers\n",
    "      self.input_size = architecture.input_size\n",
    "      self.learning_rate = architecture.learning_rate\n",
    "      self.error_threshold = architecture.error_threshold\n",
    "      self.max_iter = architecture.max_iteration\n",
    "      self.batch_size = architecture.batch_size\n",
    "      self.weights = architecture.initial_weights\n",
    "      self.expected_stopped_by = architecture.stopped_by\n",
    "      self.expect_weights = architecture.final_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocessing Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Processing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1667,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squared_error(weights1, weights2):\n",
    "    sse = 0\n",
    "    for layer1, layer2 in zip(weights1, weights2):\n",
    "        for neuron_weights1, neuron_weights2 in zip(layer1, layer2):\n",
    "            for weight1, weight2 in zip(neuron_weights1, neuron_weights2):\n",
    "                diff = weight1 - weight2\n",
    "                sse += diff * diff\n",
    "    return sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILENAME: softmax_two_layer.json\n",
      "\tINPUT SIZE: 2\n",
      "\tLAYERS: [{'number_of_neurons': 4, 'activation_function': 'relu'}, {'number_of_neurons': 2, 'activation_function': 'softmax'}]\n",
      "\tINPUT: [[3.99, 2.96], [-0.71, 2.8], [-2.43, -0.2], [-1.9, 2.62], [-2.58, 1.43], [-3.43, -0.25], [1.15, -2.3], [4.28, 3.45]]\n",
      "\tINITIAL WEIGHTS: [[[0.1, -0.1, 0.1, -0.1], [-0.1, 0.1, -0.1, 0.1], [0.1, 0.1, -0.1, -0.1]], [[0.12, -0.1], [-0.12, 0.1], [0.12, -0.1], [-0.12, 0.1], [0.02, 0.0]]]\n",
      "\tTARGET: [[0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1]]\n",
      "\tLEARNING RATE: 0.1\n",
      "\tBATCH SIZE: 1\n",
      "\tMAX ITERATION: 200\n",
      "\tERROR THRESHOLD: 0.01\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LAST ITERATION : 106\n",
      "TOTAL ERROR VAL: 0.009936242476110826\n",
      "\n",
      "\n",
      "EXCPECTED\n",
      "STOPPED BY : error_threshold\n",
      "[\n",
      "\t [-0.28730211, -0.28822282, -0.70597451, 0.42094471]\n",
      "\t [-0.5790794, -1.1836444, -1.34287961, 0.69575311]\n",
      "\t [-0.41434377, 1.51314676, -0.97649086, -1.3043465]\n",
      "]\n",
      "[\n",
      "\t [-1.72078607, 1.74078607]\n",
      "\t [-0.50352956, 0.48352956]\n",
      "\t [1.25764816, -1.23764816]\n",
      "\t [-1.16998784, 1.14998784]\n",
      "\t [1.0907634, -1.0707634]\n",
      "]\n",
      "\n",
      "\n",
      "RESULT\n",
      "STOPPED BY : ERROR THRESHOLD\n",
      "[\n",
      "\t [-0.2873021094284748, -0.2882228247672505, -0.7059745091124836, 0.42094470920655785]\n",
      "\t [-0.579079399925297, -1.1836444019669576, -1.3428796084622934, 0.6957531055595404]\n",
      "\t [-0.41434376959867897, 1.5131467608357145, -0.9764908601424802, -1.3043464969311707]\n",
      "]\n",
      "[\n",
      "\t [-1.7207860719146808, 1.7407860719146813]\n",
      "\t [-0.5035295626116902, 0.4835295626116901]\n",
      "\t [1.2576481557452117, -1.2376481557452095]\n",
      "\t [-1.1699878437697822, 1.1499878437697817]\n",
      "\t [1.0907633975375395, -1.0707633975375392]\n",
      "]\n",
      "\n",
      "\n",
      "SSE (SUM SQUARED ERROR): 1.5845503163736588e-16\n",
      "====================================================================\n",
      "====================================================================\n",
      "\n",
      "\n",
      "FILENAME: softmax_two_layer.json\n",
      "\tINPUT SIZE: 2\n",
      "\tLAYERS: [{'number_of_neurons': 4, 'activation_function': 'relu'}, {'number_of_neurons': 2, 'activation_function': 'softmax'}]\n",
      "\tINPUT: [[3.99, 2.96], [-0.71, 2.8], [-2.43, -0.2], [-1.9, 2.62], [-2.58, 1.43], [-3.43, -0.25], [1.15, -2.3], [4.28, 3.45]]\n",
      "\tINITIAL WEIGHTS: [[[0.1, -0.1, 0.1, -0.1], [-0.1, 0.1, -0.1, 0.1], [0.1, 0.1, -0.1, -0.1]], [[0.12, -0.1], [-0.12, 0.1], [0.12, -0.1], [-0.12, 0.1], [0.02, 0.0]]]\n",
      "\tTARGET: [[0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1]]\n",
      "\tLEARNING RATE: 0.1\n",
      "\tBATCH SIZE: 1\n",
      "\tMAX ITERATION: 200\n",
      "\tERROR THRESHOLD: 0.01\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LAST ITERATION : 106\n",
      "TOTAL ERROR VAL: 0.009936242476110826\n",
      "\n",
      "\n",
      "EXCPECTED\n",
      "STOPPED BY : error_threshold\n",
      "[\n",
      "\t [-0.28730211, -0.28822282, -0.70597451, 0.42094471]\n",
      "\t [-0.5790794, -1.1836444, -1.34287961, 0.69575311]\n",
      "\t [-0.41434377, 1.51314676, -0.97649086, -1.3043465]\n",
      "]\n",
      "[\n",
      "\t [-1.72078607, 1.74078607]\n",
      "\t [-0.50352956, 0.48352956]\n",
      "\t [1.25764816, -1.23764816]\n",
      "\t [-1.16998784, 1.14998784]\n",
      "\t [1.0907634, -1.0707634]\n",
      "]\n",
      "\n",
      "\n",
      "RESULT\n",
      "STOPPED BY : ERROR THRESHOLD\n",
      "[\n",
      "\t [-0.28730210942847484, -0.2882228247672504, -0.7059745091124834, 0.42094470920655785]\n",
      "\t [-0.5790793999252973, -1.1836444019669583, -1.3428796084622932, 0.6957531055595404]\n",
      "\t [-0.41434376959867913, 1.5131467608357148, -0.9764908601424801, -1.3043464969311707]\n",
      "]\n",
      "[\n",
      "\t [-1.7207860719146808, 1.7407860719146813]\n",
      "\t [-0.5035295626116906, 0.48352956261169]\n",
      "\t [1.2576481557452113, -1.237648155745209]\n",
      "\t [-1.1699878437697833, 1.1499878437697815]\n",
      "\t [1.0907633975375395, -1.0707633975375392]\n",
      "]\n",
      "\n",
      "\n",
      "SSE (SUM SQUARED ERROR): 1.5845505031256717e-16\n",
      "====================================================================\n",
      "====================================================================\n",
      "\n",
      "\n",
      "FILENAME: relu_b.json\n",
      "\tINPUT SIZE: 2\n",
      "\tLAYERS: [{'number_of_neurons': 3, 'activation_function': 'relu'}]\n",
      "\tINPUT: [[1.0, 0.8], [-0.3, -1.0]]\n",
      "\tINITIAL WEIGHTS: [[[-0.2, 0.2, 1.0], [0.3, 0.5, 0.5], [-0.5, -1.0, 0.5]]]\n",
      "\tTARGET: [[1.0, 0.1, 0.1], [0.1, 0.1, 1.0]]\n",
      "\tLEARNING RATE: 0.1\n",
      "\tBATCH SIZE: 2\n",
      "\tMAX ITERATION: 1\n",
      "\tERROR THRESHOLD: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LAST ITERATION : 0\n",
      "TOTAL ERROR VAL: 1.3967749999999999\n",
      "\n",
      "\n",
      "EXCPECTED\n",
      "STOPPED BY : max_iteration\n",
      "[\n",
      "\t [-0.211, 0.105, 0.885]\n",
      "\t [0.3033, 0.5285, 0.3005]\n",
      "\t [-0.489, -0.905, 0.291]\n",
      "]\n",
      "\n",
      "\n",
      "RESULT\n",
      "STOPPED BY : MAX ITERATION\n",
      "[\n",
      "\t [-0.21100000000000002, 0.105, 0.885]\n",
      "\t [0.3033, 0.5285, 0.3005]\n",
      "\t [-0.489, -0.905, 0.291]\n",
      "]\n",
      "\n",
      "\n",
      "SSE (SUM SQUARED ERROR): 7.703719777548943e-34\n",
      "====================================================================\n",
      "====================================================================\n",
      "\n",
      "\n",
      "FILENAME: linear_two_iteration.json\n",
      "\tINPUT SIZE: 2\n",
      "\tLAYERS: [{'number_of_neurons': 3, 'activation_function': 'linear'}]\n",
      "\tINPUT: [[3.0, 1.0], [1.0, 2.0]]\n",
      "\tINITIAL WEIGHTS: [[[0.1, 0.3, 0.2], [0.4, 0.2, -0.7], [0.1, -0.8, 0.5]]]\n",
      "\tTARGET: [[2.0, 0.3, -1.9], [1.3, -0.7, 0.1]]\n",
      "\tLEARNING RATE: 0.1\n",
      "\tBATCH SIZE: 2\n",
      "\tMAX ITERATION: 2\n",
      "\tERROR THRESHOLD: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LAST ITERATION : 1\n",
      "TOTAL ERROR VAL: 0.09092500000000003\n",
      "\n",
      "\n",
      "EXCPECTED\n",
      "STOPPED BY : max_iteration\n",
      "[\n",
      "\t [0.166, 0.338, 0.153]\n",
      "\t [0.502, 0.226, -0.789]\n",
      "\t [0.214, -0.718, 0.427]\n",
      "]\n",
      "\n",
      "\n",
      "RESULT\n",
      "STOPPED BY : MAX ITERATION\n",
      "[\n",
      "\t [0.16600000000000004, 0.338, 0.15300000000000002]\n",
      "\t [0.502, 0.22599999999999992, -0.7889999999999999]\n",
      "\t [0.21400000000000002, -0.718, 0.42700000000000005]\n",
      "]\n",
      "\n",
      "\n",
      "SSE (SUM SQUARED ERROR): 2.465190328815662e-32\n",
      "====================================================================\n",
      "====================================================================\n",
      "\n",
      "\n",
      "FILENAME: linear_small_lr.json\n",
      "\tINPUT SIZE: 2\n",
      "\tLAYERS: [{'number_of_neurons': 3, 'activation_function': 'linear'}]\n",
      "\tINPUT: [[3.0, 1.0], [1.0, 2.0]]\n",
      "\tINITIAL WEIGHTS: [[[0.1, 0.3, 0.2], [0.4, 0.2, -0.7], [0.1, -0.8, 0.5]]]\n",
      "\tTARGET: [[2.0, 0.3, -1.9], [1.3, -0.7, 0.1]]\n",
      "\tLEARNING RATE: 0.001\n",
      "\tBATCH SIZE: 2\n",
      "\tMAX ITERATION: 1\n",
      "\tERROR THRESHOLD: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LAST ITERATION : 0\n",
      "TOTAL ERROR VAL: 0.3325\n",
      "\n",
      "\n",
      "EXCPECTED\n",
      "STOPPED BY : max_iteration\n",
      "[\n",
      "\t [0.1008, 0.3006, 0.1991]\n",
      "\t [0.402, 0.201, -0.7019]\n",
      "\t [0.101, -0.799, 0.4987]\n",
      "]\n",
      "\n",
      "\n",
      "RESULT\n",
      "STOPPED BY : MAX ITERATION\n",
      "[\n",
      "\t [0.10120000000000001, 0.3006, 0.1991]\n",
      "\t [0.40240000000000004, 0.201, -0.7018999999999999]\n",
      "\t [0.10180000000000002, -0.799, 0.4987]\n",
      "]\n",
      "\n",
      "\n",
      "SSE (SUM SQUARED ERROR): 9.600000000000327e-07\n",
      "====================================================================\n",
      "====================================================================\n",
      "\n",
      "\n",
      "FILENAME: softmax.json\n",
      "\tINPUT SIZE: 8\n",
      "\tLAYERS: [{'number_of_neurons': 3, 'activation_function': 'softmax'}]\n",
      "\tINPUT: [[-2.4, -2.78, -0.6, 0.37, 2.46, -0.92, 2.76, 2.62], [-1.79, 1.65, -0.77, -1.03, 0.1, 2.12, -2.36, 1.25], [1.65, 2.34, 0.27, 2.34, 0.52, 1.37, 1.77, 0.62]]\n",
      "\tINITIAL WEIGHTS: [[[0.1, 0.9, -0.1], [-0.2, 0.8, 0.2], [0.3, -0.7, 0.3], [0.4, 0.6, -0.4], [0.5, 0.5, 0.5], [-0.6, 0.4, 0.6], [-0.7, -0.3, 0.7], [0.8, 0.2, -0.8], [0.9, -0.1, 0.0]]]\n",
      "\tTARGET: [[0, 1, 0], [1, 0, 0], [0, 0, 1]]\n",
      "\tLEARNING RATE: 0.01\n",
      "\tBATCH SIZE: 1\n",
      "\tMAX ITERATION: 10\n",
      "\tERROR THRESHOLD: 0.05\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LAST ITERATION : 9\n",
      "TOTAL ERROR VAL: 0.8224087756463515\n",
      "\n",
      "\n",
      "EXCPECTED\n",
      "STOPPED BY : max_iteration\n",
      "[\n",
      "\t [0.12674605, 0.9149538, -0.14169985]\n",
      "\t [-0.33551647, 0.67700488, 0.45851159]\n",
      "\t [0.48314436, -0.85241216, 0.2692678]\n",
      "\t [0.3400255, 0.57237542, -0.31240092]\n",
      "\t [0.31397716, 0.46349737, 0.72252547]\n",
      "\t [-0.69652442, 0.4789189, 0.61760552]\n",
      "\t [-0.50884515, -0.36354141, 0.57238656]\n",
      "\t [0.41891295, 0.26354517, -0.48245812]\n",
      "\t [0.90374164, -0.01759501, -0.08614663]\n",
      "]\n",
      "\n",
      "\n",
      "RESULT\n",
      "STOPPED BY : MAX ITERATION\n",
      "[\n",
      "\t [0.1267460546396248, 0.9149537996911864, -0.14169985433081125]\n",
      "\t [-0.33551647297979087, 0.6770048848163939, 0.4585115881633968]\n",
      "\t [0.4831443627109017, -0.8524121579144623, 0.26926779520356103]\n",
      "\t [0.3400255028017777, 0.5723754169791629, -0.3124009197809405]\n",
      "\t [0.3139771631220808, 0.4634973686971105, 0.7225254681808086]\n",
      "\t [-0.6965244228677997, 0.47891890125826836, 0.617605521609532]\n",
      "\t [-0.5088451517488036, -0.3635414093560929, 0.572386561104897]\n",
      "\t [0.41891294634289283, 0.2635451692222726, -0.4824581155651655]\n",
      "\t [0.9037416396108255, -0.017595010746242833, -0.0861466288645826]\n",
      "]\n",
      "\n",
      "\n",
      "SSE (SUM SQUARED ERROR): 1.950457685890179e-16\n",
      "====================================================================\n",
      "====================================================================\n",
      "\n",
      "\n",
      "FILENAME: linear.json\n",
      "\tINPUT SIZE: 2\n",
      "\tLAYERS: [{'number_of_neurons': 3, 'activation_function': 'linear'}]\n",
      "\tINPUT: [[3.0, 1.0], [1.0, 2.0]]\n",
      "\tINITIAL WEIGHTS: [[[0.1, 0.3, 0.2], [0.4, 0.2, -0.7], [0.1, -0.8, 0.5]]]\n",
      "\tTARGET: [[2.0, 0.3, -1.9], [1.3, -0.7, 0.1]]\n",
      "\tLEARNING RATE: 0.1\n",
      "\tBATCH SIZE: 2\n",
      "\tMAX ITERATION: 1\n",
      "\tERROR THRESHOLD: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LAST ITERATION : 0\n",
      "TOTAL ERROR VAL: 0.3325\n",
      "\n",
      "\n",
      "EXCPECTED\n",
      "STOPPED BY : max_iteration\n",
      "[\n",
      "\t [0.22, 0.36, 0.11]\n",
      "\t [0.64, 0.3, -0.89]\n",
      "\t [0.28, -0.7, 0.37]\n",
      "]\n",
      "\n",
      "\n",
      "RESULT\n",
      "STOPPED BY : MAX ITERATION\n",
      "[\n",
      "\t [0.22000000000000003, 0.36000000000000004, 0.11000000000000001]\n",
      "\t [0.64, 0.30000000000000004, -0.89]\n",
      "\t [0.28, -0.7, 0.37]\n",
      "]\n",
      "\n",
      "\n",
      "SSE (SUM SQUARED ERROR): 7.125940794232773e-33\n",
      "====================================================================\n",
      "====================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read all files inside the test folders\n",
    "import os\n",
    "\n",
    "currDir = os.getcwd()\n",
    "testDir = currDir.replace(\"src\", \"test\")\n",
    "modelsDir = currDir.replace(\"src\", \"opt\")\n",
    "\n",
    "testFiles = os.listdir(testDir)\n",
    "testFilesArr = []\n",
    "\n",
    "for filename in testFiles:\n",
    "  if (\".json\" in filename):\n",
    "    testFilesArr.append(os.path.join(testDir, filename))\n",
    "\n",
    "\n",
    "# Use every file inside testFilesArr as input\n",
    "\n",
    "for testFile in testFilesArr:\n",
    "  filename = os.path.basename(testFile)\n",
    "  print(\"FILENAME:\", filename)\n",
    "  architecture = JsonModelParser(testFile)\n",
    "  architecture.printDetails()\n",
    "  print(\"\\n\")\n",
    "  model = ArtificialNeuralNetwork(architecture)\n",
    "  model.train()\n",
    "  model.save_model(os.path.join(modelsDir, filename))\n",
    "\n",
    "  error = sum_squared_error(model.weights, architecture.final_weights)\n",
    "\n",
    "  print(\"\\n\")\n",
    "  print(\"SSE (SUM SQUARED ERROR):\", error)\n",
    "  print(\"====================================================================\")\n",
    "  print(\"====================================================================\")\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1669,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRIS\n",
    "\n",
    "inputs = []\n",
    "targets = []\n",
    "with open(\"iris.csv\", \"r\") as csvfile:\n",
    "    # Gunakan csv.DictReader\n",
    "    datareader = csv.DictReader(csvfile)\n",
    "\n",
    "    # Tidak perlu next(datareader), DictReader sudah meng-handle header\n",
    "    for row in datareader:\n",
    "        # Sekarang row adalah dictionary, dan Anda bisa mengakses nilai menggunakan nama kolom\n",
    "        current = [\n",
    "            float(row[\"SepalLengthCm\"]),\n",
    "            float(row[\"SepalWidthCm\"]),\n",
    "            float(row[\"PetalLengthCm\"]),\n",
    "            float(row[\"PetalWidthCm\"])\n",
    "        ]\n",
    "        species = row[\"Species\"]\n",
    "        if(species == \"Iris-setosa\"):\n",
    "            target = [1,0,0]\n",
    "        elif(species == \"Iris-versicolor\"):\n",
    "            target = [0,1,0]\n",
    "        elif(species == \"Iris-virginica\"):\n",
    "            target = [0,0,1]\n",
    "        inputs.append(current)\n",
    "        targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LAST ITERATION : 999\n",
      "TOTAL ERROR VAL: 0.10799645198590958\n",
      "\n",
      "\n",
      "EXCPECTED\n",
      "STOPPED BY : \n",
      "\n",
      "\n",
      "RESULT\n",
      "STOPPED BY : MAX ITERATION\n",
      "[\n",
      "\t [0.587073476645592, 0.8980810347582467, -1.1851545114038078]\n",
      "\t [1.1208595088687452, 0.9597290374574284, -1.7805885463261513]\n",
      "\t [2.430619599353256, -0.028021295700770478, -2.102598303652427]\n",
      "\t [-3.075217470038597, -0.22797076876503622, 3.6031882388036136]\n",
      "\t [-1.383558506239567, -1.219106126141017, 2.9026646323805902]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "architecture = JsonModelParser(\"iris_config.json\")\n",
    "architecture.input_size = len(inputs[0])\n",
    "architecture.input = inputs\n",
    "architecture.target = targets\n",
    "model = ArtificialNeuralNetwork(architecture)\n",
    "model.train()\n",
    "model.save_model(os.path.join(os.getcwd().replace(\"src\", \"opt\"), \"iris_train.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1671,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-setosa',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-virginica',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-virginica',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-versicolor',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica',\n",
       " 'Iris-virginica']"
      ]
     },
     "execution_count": 1671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = ArtificialNeuralNetwork(None)\n",
    "loaded_model.load_model(os.path.join(modelsDir, \"iris_train.json\"))\n",
    "loaded_model.input_data = inputs\n",
    "loaded_model.target = targets\n",
    "res_code = loaded_model.predict()\n",
    "res = []\n",
    "for i in range (len(res_code)):\n",
    "    idmax = np.argmax(res_code[i])\n",
    "    if idmax == 0:\n",
    "        res.append('Iris-setosa')\n",
    "    elif idmax == 1:\n",
    "        res.append('Iris-versicolor')\n",
    "    else:\n",
    "        res.append('Iris-virginica')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1672,
   "metadata": {},
   "outputs": [],
   "source": [
    "currDir = os.getcwd()\n",
    "testDir = currDir.replace(\"src\", \"test\")\n",
    "modelsDir = currDir.replace(\"src\", \"opt\")\n",
    "\n",
    "loaded_model = ArtificialNeuralNetwork(None)\n",
    "loaded_model.load_model(os.path.join(modelsDir, \"linear_small_lr.json\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
