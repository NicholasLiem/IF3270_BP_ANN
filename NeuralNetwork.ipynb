{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection    import train_test_split\n",
    "from sklearn.preprocessing      import LabelEncoder\n",
    "from sklearn.metrics            import accuracy_score\n",
    "from joblib                     import dump, load\n",
    "from datetime                   import datetime\n",
    "from Utils.ActivationFunction   import ActivationFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_data, learning_rate, err_threshold, max_iter, batch_size, debug = True):\n",
    "        self.input_data = input_data\n",
    "        self.layers = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.err_threshold = err_threshold\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.debug = debug\n",
    "\n",
    "    def add_layer(self, n_inputs: int, n_neurons: int, activ_func):\n",
    "        self.layers.append(NeuralLayer(n_inputs, n_neurons, activ_func))\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "        raise Exception(\"Haven't checked this again\")\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            inputs = layer.feed_forward(inputs)\n",
    "            if self.debug:\n",
    "                print(f'Layer {i+1}, Output: {inputs}')\n",
    "        return inputs\n",
    "    \n",
    "    def backward_propagate(self, target_output):\n",
    "        raise Exception(\"TODO: Check this please, kyknya masih salah\")\n",
    "        # Error term for each output unit j\n",
    "        # Delta j = oj (1 - oj) (tj - oj)\n",
    "        for i, output_neuron in enumerate(self.layers[-1].neurons):\n",
    "            error = target_output[i] - output_neuron.output\n",
    "            output_neuron.calculate_delta(error)\n",
    "        \n",
    "        # Error term for each output unit l\n",
    "        # Delta h = oh (1 - oh) sig(wlh * delta l)\n",
    "        for l in range(len(self.layers) -2, -1, -1):\n",
    "            layer = self.layers[l]\n",
    "            next_layer = self.layers[l+1]\n",
    "\n",
    "            for j, neuron in enumerate(layer.neurons):\n",
    "                error = sum([next_neuron.weights[j] * next_neuron.delta for next_neuron in next_layer.neurons])\n",
    "                neuron.calculate_delta(error)\n",
    "    \n",
    "    def calculate_total_error(self, outputs, targets):\n",
    "        raise Exception(\"Haven't checked this again\")\n",
    "        mse = np.mean([(t - o)**2 for t, o in zip(targets, outputs)])\n",
    "        return mse\n",
    "\n",
    "    def update_weights(self):\n",
    "        for layer in self.layers:\n",
    "            layer.update_weights(self.learning_rate)\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        return self.forward_propagate(input_data)\n",
    "\n",
    "    def train(self):\n",
    "        for itr in range(self.max_iter):\n",
    "            total_error = 0\n",
    "            \n",
    "            for i in range(0, len(self.input_data), self.batch_size):\n",
    "                batch = self.input_data[i:i+self.batch_size]\n",
    "                inputs, targets = zip(*batch)\n",
    "                \n",
    "                outputs = self.forward_propagate(inputs)\n",
    "                self.backward_propagate(targets)\n",
    "                self.update_weights()\n",
    "\n",
    "                batch_error = self.calculate_total_error(outputs, targets)\n",
    "                total_error += batch_error\n",
    "            \n",
    "            # Ambil nilai avg dari total per batch\n",
    "            total_error /= len(self.input_data) / self.batch_size\n",
    "\n",
    "            if self.debug:\n",
    "                print(f\"Epoch {itr+1}/{self.max_iter}, Total Error: {total_error}\")\n",
    "            \n",
    "            # Stop training kalau err_threshold tercapai\n",
    "            if total_error <= self.err_threshold:\n",
    "                print(f\"Training complete at epoch {itr+1} with total error {total_error} below threshold {self.err_threshold}.\")\n",
    "                break\n",
    "\n",
    "class NeuralLayer:\n",
    "    __counter = 0\n",
    "    def __init__(self, n_inputs, n_neurons, activ_func):\n",
    "        NeuralLayer.__counter += 1\n",
    "        self.__neurons = [Neuron(n_inputs, activ_func) for _ in range(n_neurons)]\n",
    "        self.__counter = NeuralLayer.__counter\n",
    "\n",
    "    @property\n",
    "    def neurons(self):\n",
    "        return self.__neurons\n",
    "\n",
    "    @property\n",
    "    def deltas(self):\n",
    "        return [i.delta for i in self.neurons]\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        return [neuron.calculate_output(inputs) for neuron in self.neurons]\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.update_weights(learning_rate)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'---> Layer {self.__counter}, # of neurons: {len(self.neurons)}'\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, n_weights: int, activ_func, bias: float = 1.0):\n",
    "        # Weight initialization? I use random here\n",
    "        self.__weights = np.random.rand(n_weights)\n",
    "        self.__bias = bias\n",
    "        self.__activation = activ_func\n",
    "        self.__output = 0\n",
    "        self.__inputs = []\n",
    "        self.__delta = 0\n",
    "        self.__n_weights = n_weights\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        return self.__output\n",
    "    \n",
    "    @property\n",
    "    def delta(self):\n",
    "        return self.__delta\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.__weights\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        # Kalau pakai bias in each neuron\n",
    "        # weighted_sum = np.dot(inputs, self.__weights) + self.__bias\n",
    "        weighted_sum = np.dot(inputs, self.__weights)\n",
    "        self.__output = self.__activation(weighted_sum)\n",
    "        self.__inputs = inputs\n",
    "        return self.__output\n",
    "\n",
    "    def calculate_delta(self, error):\n",
    "        self.__delta = error * self.__activation.dfunc(self.__output)\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        for i in range(self.__n_weights):\n",
    "            self.__weights[i] += learning_rate * self.__delta * self.__inputs[i]\n",
    "        # self.__bias += learning_rate * self.__delta\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '--- weights = {}, bias = {}'.format(self.__weights, self.__bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "data = pd.read_csv('Datasets/iris.csv')\n",
    "\n",
    "# Encode target label to integer\n",
    "le = LabelEncoder()\n",
    "data['Species'] = le.fit_transform(data['Species'])\n",
    "\n",
    "# Remove id and species column and get species column data\n",
    "X = data.drop(['Id', 'Species'], axis=1)\n",
    "y = data['Species']\n",
    "\n",
    "# Splitting Training and Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NeuralNetwork class\n",
    "nn = NeuralNetwork(input_data=X_train, learning_rate=0.01, err_threshold=0.01, max_iter=1000, batch_size=32, debug=True)\n",
    "\n",
    "# Adding the layers\n",
    "nn.add_layer(n_inputs=2, n_neurons=3, activ_func=ActivationFunction(types='Linear'))\n",
    "nn.add_layer(n_inputs=3, n_neurons=1, activ_func=ActivationFunction(types='Sigmoid'))\n",
    "\n",
    "# Train the NeuralNetwork\n",
    "nn.train()\n",
    "\n",
    "# Dump the model\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "file_name = f'Models/NeuralNetwork_{timestamp}.joblib'\n",
    "dump(nn, file_name)\n",
    "print(f\"Model saved successfully as {file_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction based on the trained model\n",
    "y_pred = nn.predict()\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
